{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dca243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f24aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c723c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = open('Gaurav Suri.pdf', 'rb') #resume input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e172c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfReader = PyPDF2.PdfReader(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a7e4ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Author': 'Prashant Upadhyay',\n",
       " '/Creator': 'Microsoft® Word 2016',\n",
       " '/CreationDate': \"D:20230211172458+00'00'\",\n",
       " '/Producer': 'www.ilovepdf.com',\n",
       " '/ModDate': 'D:20230211172458Z'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfReader.metadata #About PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7265cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "resume_path = r'C:\\Users\\PRASHANT\\Untitled Folder 3\\resume\\pu.pdf'\n",
    "resume_text = ''\n",
    "\n",
    "with open(resume_path, 'rb') as pdf_file:\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "    for page in range(len(pdf_reader.pages)):\n",
    "        resume_text += pdf_reader.pages[page].extract_text ()\n",
    "\n",
    "# # Step 2: Tokenization\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# tokenized_resume = tokenizer.tokenize(resume_text)\n",
    "\n",
    "# # Step 3: Encoding\n",
    "# input_ids = tokenizer.encode(tokenized_resume, add_special_tokens=True, max_length=512, truncation=True, padding=True, return_tensors='tf')\n",
    "# token_type_ids = tf.zeros_like(input_ids)\n",
    "# attention_mask = tf.ones_like(input_ids)\n",
    "\n",
    "# # Step 4: BERT Model\n",
    "# bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "# encoded_resume = bert_model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "929a2038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CAREER  OBJECTIVE  \\nTraining  and Certifications  \\nJobs  and Responsibilities  \\nCOMPUTER  SKILLS  \\nACADEMIC  BACKGROUND  PRASHANT  UPADHYAY  \\nPhone: +91  8954653528  \\nEmail:  prashantupadhyay0@gmail.com  \\nPrashant  is a career -oriented  professional  with decent  communication  and interpersonal  skills,  who knows  \\nhow to make  sense  of data and translate  it into actionable  insights.  He is familiar  with gathering,  cleaning,  \\nand organizing data for the use of technical and non -technical personnel. He is seeking a challenging  \\nposition in a growth -oriented organization w here he can effectively contribute through his skills and  \\nabilities.  \\n \\nCertifications:  \\nInstitution:  Google  May,  2022  \\nCertificate:  Google  Data  analytics  professional  certification  \\n \\nInstitution:  LinkedIn  Jul, 2021  \\nCertificate:  Tableau  Essential  Training  (2020.1)  \\nZummit Infolabs  Nov 2022  – Present  \\nJr. Data  Scientist  \\n\\uf0b7 Writing  code  in Python , Tensorflow /Keras  projects  related  to CNN , GAN , and RNN  for the Hospitality  \\nor Financial  domain  \\n\\uf0b7 Used Jupyter /Collab  notebooks  for Machine  Learning  / Deep  Learning  problems  \\nYoshops.com  Jun 2022  – Aug 2022  \\nData  Science  Intern  \\n\\uf0b7 Undertook  to process,  structured  and unstructured  data and analyze  large  amounts  of information  to \\ndiscover  trends  and patterns  \\n\\uf0b7 Combined  models  through  ensemble  modeling  Present  information  using  data visualization  techniques  \\nwith the help of Plotly , Seaborn,  and Matplotlib  \\n\\uf0b7 Web  Scraping  using  Python,  EDA on real-time data,  Implementing  the \"GPT2  model \" for auto- \\ngenerating  text, Handling  Large Real -world  Datasets,  and Creating  .exe files  from Python scripting  \\n \\nVariedaperture.com  Dec 2018  - Nov 2020  \\nWordPress  Developer/Blogger/  Owner  \\n\\uf0b7 Designed and manipulated the website front -end with the help of front -end languages such as HTML,  \\nJavaScript,  and CSS \\n \\nCISPL  Global,  Gurgaon  Mar 2017  - Aug 2018  \\nJr. US IT Recruiter  \\n\\uf0b7 Prepared  weekly  reports  on recruiting  activities,  candidate  placements  and job openings  \\n\\uf0b7 Handled  the tasks  of screening,  selecting,  and submitting  candidates  to job orders  within  a defined  \\ndiscipline  \\n \\n \\nSoftware: Tableau Public, Jupyter Notebook, MySQL , Microsoft Excel, Microsoft word, Kaggle  \\nLanguages: Python , C, C++,  R programming,  SQL \\n \\n \\nB. TECH (Computer Science Engineering with specialization in Mainframe technologies),  \\nUniversity  of Petroleum  and Energy  Studies,  Dehradun  JULY  2017  '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa92e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAREER  OBJECTIVE  Training  and Certifications  Jobs  and Responsibilities  COMPUTER  SKILLS  ACADEMIC  BACKGROUND  PRASHANT  UPADHYAY  Phone: +91  8954653528  Email:  prashantupadhyay0@gmail.com  Prashant  is a career -oriented  professional  with decent  communication  and interpersonal  skills,  who knows  how to make  sense  of data and translate  it into actionable  insights.  He is familiar  with gathering,  cleaning,  and organizing data for the use of technical and non -technical personnel. He is seeking a challenging  position in a growth -oriented organization w here he can effectively contribute through his skills and  abilities.   Certifications:  Institution:  Google  May,  2022  Certificate:  Google  Data  analytics  professional  certification   Institution:  LinkedIn  Jul, 2021  Certificate:  Tableau  Essential  Training  (2020.1)  Zummit Infolabs  Nov 2022  – Present  Jr. Data  Scientist   Writing  code  in Python , Tensorflow /Keras  projects  related  to CNN , GAN , and RNN  for the Hospitality  or Financial  domain   Used Jupyter /Collab  notebooks  for Machine  Learning  / Deep  Learning  problems  Yoshops.com  Jun 2022  – Aug 2022  Data  Science  Intern   Undertook  to process,  structured  and unstructured  data and analyze  large  amounts  of information  to discover  trends  and patterns   Combined  models  through  ensemble  modeling  Present  information  using  data visualization  techniques  with the help of Plotly , Seaborn,  and Matplotlib   Web  Scraping  using  Python,  EDA on real-time data,  Implementing  the \"GPT2  model \" for auto- generating  text, Handling  Large Real -world  Datasets,  and Creating  .exe files  from Python scripting   Variedaperture.com  Dec 2018  - Nov 2020  WordPress  Developer/Blogger/  Owner   Designed and manipulated the website front -end with the help of front -end languages such as HTML,  JavaScript,  and CSS  CISPL  Global,  Gurgaon  Mar 2017  - Aug 2018  Jr. US IT Recruiter   Prepared  weekly  reports  on recruiting  activities,  candidate  placements  and job openings   Handled  the tasks  of screening,  selecting,  and submitting  candidates  to job orders  within  a defined  discipline    Software: Tableau Public, Jupyter Notebook, MySQL , Microsoft Excel, Microsoft word, Kaggle  Languages: Python , C, C++,  R programming,  SQL   B. TECH (Computer Science Engineering with specialization in Mainframe technologies),  University  of Petroleum  and Energy  Studies,  Dehradun  JULY  2017  \n"
     ]
    }
   ],
   "source": [
    "# text = \"This is a sample string with\\na newline character.\"\n",
    "resume_text = resume_text.replace(\"\\n\", \"\")\n",
    "print(resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "228b7215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "stop_words = set(stopwords.words('english'))\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
    "resume_text = pattern.sub('', resume_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478d10f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fca995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96c153dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 6: Evaluation\n",
    "# # Evaluate the performance of the model on a validation set\n",
    "\n",
    "# # Prepare a validation set\n",
    "\n",
    "# validation_text = resume_text\n",
    "\n",
    "# # Tokenize and encode the validation set\n",
    "# tokenized_validation = tokenizer.tokenize(validation_text)\n",
    "# input_ids_val = tokenizer.encode(tokenized_validation, add_special_tokens=True, max_length=512, truncation=True, padding=True, return_tensors='tf')\n",
    "# token_type_ids_val = tf.zeros_like(input_ids_val)\n",
    "# attention_mask_val = tf.ones_like(input_ids_val)\n",
    "\n",
    "# # Use the encoded validation set to make predictions using the BERT model\n",
    "# encoded_validation = bert_model(input_ids_val, token_type_ids=token_type_ids_val, attention_mask=attention_mask_val)\n",
    "\n",
    "# # Perform text classification to extract information from the encoded validation set\n",
    "# validation_summary = []\n",
    "# for i in range(len(tokenized_validation)):\n",
    "#     if tokenized_validation[i] == 'years' or tokenized_validation[i] == 'degree' or tokenized_validation[i] == 'experience' or tokenized_validation[i] == 'programming':\n",
    "#         validation_summary.append(tokenized_validation[i])\n",
    "#         j = i + 1\n",
    "#         while j < len(tokenized_validation) and tokenized_validation[j] != 'skills' and tokenized_validation[j] != 'degree' and tokenized_validation[j] != 'experience' and tokenized_validation[j] != 'programming':\n",
    "#             validation_summary[-1] += ' ' + tokenized_validation[j]\n",
    "#             j += 1\n",
    "\n",
    "# # Evaluate the performance of the model on the validation set\n",
    "# # For example, we can print the extracted information from the validation set and compare it with the ground truth\n",
    "# # print(\"Extracted information from validation set:\")\n",
    "# print(validation_summary)\n",
    "# # print(\"Ground truth:\")\n",
    "# # print([\"years of experience in Java programming\", \"Bachelor's degree in Computer Science and Engineering\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead9a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6418d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entry-level\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Define the categories\n",
    "categories = {\n",
    "    'Entry-level': 0,\n",
    "    'Mid-level': 1,\n",
    "    'Senior-level': 2\n",
    "}\n",
    "\n",
    "# Define a function to classify a resume\n",
    "def classify_resume(resume):\n",
    "    # Tokenize the resume\n",
    "    input_ids = tokenizer.encode(resume, add_special_tokens=True, truncation=True, max_length=512)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Run the resume through the model\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "    prediction = logits.argmax(dim=-1).item()\n",
    "\n",
    "    # Map the prediction to the corresponding category\n",
    "    for category, label in categories.items():\n",
    "        if label == prediction:\n",
    "            return category\n",
    "    return None\n",
    "\n",
    "# Test the function with a sample resume\n",
    "resume = resume_text\n",
    "category = classify_resume(resume)\n",
    "print(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae633c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "696ff167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] career objective training certification ##s jobs responsibilities computer skills', 'background pr', '##hya ##y phone : +', '##54 ##65 ##35 ##28 email :', '##hya', 'com', 'decent', 'skills , knows', 'translate', 'insights . he familiar gathering , cleaning', 'organizing', 'non -', 'personnel . he seeking', 'position growth', 'oriented organization w effectively', 'abilities . certification ##s : institution : google may ,', '##2 certificate : google data analytics', 'certification', ': linked', 'jul', '2021 certificate : table ##au essential training (', ') zu', '##it', '##lab ##s nov', '##2 – present jr . data scientist writing code python ,', '##flow / ke ##ras projects related cnn , gan , rn ##n hospitality financial domain used ju', '/', '##lab notebook ##s', 'learning / deep learning problems yo ##sho ##ps', 'com jun', '##2 – aug', '##2 data science intern undertook process , structured un', 'analyze', 'amounts', 'discover trends patterns combined models ensemble modeling', 'using', '##ization techniques', '##ly , sea ##born ,', '##pl', '##lib', 'scraping using', ',', ', implementing', '\"', '- generating', ', handling', ', creating', 'ex', '##ing varied', '##ure', 'com dec', '- nov', '/ blogger', 'owner designed manipulated website', 'front - end languages', 'cs', 'cis ##pl global , gu ##rga', 'mar', '-', 'jr . us it recruit ##er prepared weekly reports recruiting activities ,', '##s', 'openings', 'screening', '##ting candidates', 'orders', 'software : table ##au public , ju', 'notebook', 'languages :', ',', 'programming , sql b .', '( computer science engineering', 'main ##frame technologies', ', university petroleum energy studies , de ##hra ##dun july', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# Define the categories\n",
    "categories = {\n",
    "    'B-KEYWORD': 1,  # Beginning of a keyword\n",
    "    'I-KEYWORD': 2,  # Inside a keyword\n",
    "    'O': 0,          # Outside a keyword\n",
    "}\n",
    "\n",
    "# Define a function to extract keywords from a resume\n",
    "def extract_keywords(resume):\n",
    "    # Tokenize the resume\n",
    "    input_ids = tokenizer.encode(resume, add_special_tokens=True, truncation=True, max_length=512)\n",
    "    input_ids = torch.tensor(input_ids).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Run the resume through the model\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "    predictions = logits.argmax(dim=-1).tolist()[0]\n",
    "\n",
    "    # Map the predictions to keywords\n",
    "    keywords = []\n",
    "    current_keyword = []\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if prediction == categories['B-KEYWORD']:\n",
    "            current_keyword.append(tokenizer.convert_ids_to_tokens([input_ids[0][i].item()])[0])\n",
    "        elif prediction == categories['I-KEYWORD']:\n",
    "            current_keyword.append(tokenizer.convert_ids_to_tokens([input_ids[0][i].item()])[0])\n",
    "        else:\n",
    "            if current_keyword:\n",
    "                keywords.append(' '.join(current_keyword))\n",
    "                current_keyword = []\n",
    "    if current_keyword:\n",
    "        keywords.append(' '.join(current_keyword))\n",
    "    return keywords\n",
    "\n",
    "# Test the function with a sample resume\n",
    "resume = resume_text\n",
    "keywords = extract_keywords(resume)\n",
    "print(keywords)  # Output: ['experienced', 'software developer', 'degree', 'computer science']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e0335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df0406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95464eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58663df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de72b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62924dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
